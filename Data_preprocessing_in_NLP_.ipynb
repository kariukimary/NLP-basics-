{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kariukimary/NLP-basics-/blob/main/Data_preprocessing_in_NLP_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Getting started  with NLP**"
      ],
      "metadata": {
        "id": "2ChGi3eiHX3C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiwOIbPWjmfB"
      },
      "source": [
        "# **Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeN_aGGU1zK-",
        "outputId": "f5f39776-f82e-4691-a38d-fe4d971f342e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dr.muguro\n",
            "is\n",
            "the\n",
            "one\n",
            "who\n",
            "'s\n",
            "is\n",
            "guding\n",
            "us\n",
            "on\n",
            "how\n",
            "we\n",
            "will\n",
            "perofrm\n",
            "african\n",
            "next\n",
            "voices\n",
            "data\n",
            "collection\n",
            " \n",
            "in\n",
            "kikuyu\n",
            "team\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "# creating a blank english component and a doc  which has your text\n",
        "comp=spacy.blank(\"en\")\n",
        "# word tokenization\n",
        "doc=comp(\"Dr.muguro is the one who's  guding us on how we will perofrm african next voices data collection  in kikuyu team.\")\n",
        "\n",
        "for  text  in doc:\n",
        "  print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9drsYOT04jqy",
        "outputId": "508e68c7-d92b-480a-b19f-97ffc100b7dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "one"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# getting the tokenized word\n",
        "doc[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mun7OJw55Zs4",
        "outputId": "d70cfe5d-15ee-4a92-da3b-66c7a0e9039e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "zPMRnUJd_Es1",
        "outputId": "fe2bf83f-bad5-4847-d6a0-100eda24ee1c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>spacy.lang.en.English</b><br/>def __call__(text: Union[str, Doc], *, disable: Iterable[str]=SimpleFrozenList(), component_cfg: Optional[Dict[str, Dict[str, Any]]]=None) -&gt; Doc</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/spacy/lang/en/__init__.py</a>A text-processing pipeline. Usually you&#x27;ll load this once per process,\n",
              "and pass the instance around your application.\n",
              "\n",
              "Defaults (class): Settings, data and factory methods for creating the `nlp`\n",
              "    object and processing pipeline.\n",
              "lang (str): IETF language code, such as &#x27;en&#x27;.\n",
              "\n",
              "DOCS: https://spacy.io/api/language</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 22);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "spacy.lang.en.English"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(comp)\n",
        "# meaning its a spacy english object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "collapsed": true,
        "id": "4zqHMXCQ_NBr",
        "outputId": "a070732c-0dba-4427-b7f4-a6197723d074"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-e0526f4ba42d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#tokenixing sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dr.muguro is the one who's  guding us on how we will perofrm african next voices data collection  in kikuyu team. we are happy working with him\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36msents\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
          ]
        }
      ],
      "source": [
        "#tokenixing sentences\n",
        "nlp=comp(\"Dr.muguro is the one who is guding us on how we will perofrm african next voices data collection  in kikuyu team. we are happy working with him\")\n",
        "for sentences in nlp.sents:\n",
        "  print(sentences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpAaAqUXftNv"
      },
      "source": [
        "# **Stemming and lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miraoQgRSzuv"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_8WFeg6f7vi",
        "outputId": "d5d2f4d8-bf9c-40f2-e81d-d3bac7391e1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eating   eat\n",
            "eats   eat\n",
            "eat   eat\n",
            "adjustable   adjust\n",
            "ate   ate\n",
            "ability   abil\n",
            "meeting   meet\n"
          ]
        }
      ],
      "source": [
        "#stemming\n",
        "stemmer=PorterStemmer()\n",
        "words=[\"eating\",\"eats\",\"eat\",\"adjustable\", \"ate\",\"ability\",\"meeting\"]\n",
        "for word in  words:\n",
        "  print(word,\" \", stemmer.stem(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBNwBsKUhhGN"
      },
      "source": [
        "the above output for the words like ate and ability their outputs are not the base words, this is because stemming just uses simple  heuristic rues unlike lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81GpQs4AhIDx",
        "outputId": "2f221921-18e5-485c-dfe3-31ad1fd72547"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eating  eat\n",
            "eats  eat\n",
            "eat  eat\n",
            "adjustable  adjustable\n",
            "good  good\n",
            "better  well\n",
            "ate  eat\n",
            "ability  ability\n",
            "meeting  meeting\n"
          ]
        }
      ],
      "source": [
        "#lemmatization\n",
        "lem=spacy.load(\"en_core_web_sm\")\n",
        "document=lem(\"eating eats eat adjustable good better ate ability meeting\")\n",
        "for words in document:\n",
        "  print(words,\"\", words.lemma_)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ5m8MLKiu_a"
      },
      "source": [
        "The output above the  for the word ate it gies its base word as eat,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6EmRHtovXbj"
      },
      "source": [
        "# **Part of speech (POS) tagging**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpGGBzR-jAJb",
        "outputId": "2d973b81-0152-4fd0-fde5-7a073c471d51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "Wow\n",
            "!\n",
            "Elon\n",
            "flew\n",
            "to\n",
            "mars\n",
            "yesterday\n",
            ".\n",
            "He\n",
            "carried\n",
            "biryani\n",
            "masala\n",
            "with\n",
            "him\n"
          ]
        }
      ],
      "source": [
        "data=spacy.load(\"en_core_web_sm\")\n",
        "doc=data(\" Wow! Elon flew to mars yesterday. He carried biryani masala with him\")\n",
        "for words in doc:\n",
        "  print( words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF9e52W_v7MB",
        "outputId": "69212979-1c23-4b68-c0dd-59832f65b661"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    SPACE\n",
            "Wow   INTJ\n",
            "!   PUNCT\n",
            "Elon   PROPN\n",
            "flew   VERB\n",
            "to   ADP\n",
            "mars   NOUN\n",
            "yesterday   NOUN\n",
            ".   PUNCT\n",
            "He   PRON\n",
            "carried   VERB\n",
            "biryani   ADJ\n",
            "masala   NOUN\n",
            "with   ADP\n",
            "him   PRON\n"
          ]
        }
      ],
      "source": [
        "#printing part of speech for each token/word\n",
        "for words in doc:\n",
        "  print(words,\" \", words.pos_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drokS7tEwJUG",
        "outputId": "d26d4a42-edc4-4fd2-9dfa-3eed0c7b084f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    SPACE   space\n",
            "Wow   INTJ   interjection\n",
            "!   PUNCT   punctuation\n",
            "Elon   PROPN   proper noun\n",
            "flew   VERB   verb\n",
            "to   ADP   adposition\n",
            "mars   NOUN   noun\n",
            "yesterday   NOUN   noun\n",
            ".   PUNCT   punctuation\n",
            "He   PRON   pronoun\n",
            "carried   VERB   verb\n",
            "biryani   ADJ   adjective\n",
            "masala   NOUN   noun\n",
            "with   ADP   adposition\n",
            "him   PRON   pronoun\n"
          ]
        }
      ],
      "source": [
        "for words in doc:\n",
        "  print(words,\" \", words.pos_,\" \" ,spacy.explain(words.pos_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3ofJixkxD73",
        "outputId": "a30b8d9b-4751-44e5-8cf2-fcb7a83a7405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    SPACE   space   _SP   whitespace\n",
            "Wow   INTJ   interjection   UH   interjection\n",
            "!   PUNCT   punctuation   .   punctuation mark, sentence closer\n",
            "Elon   PROPN   proper noun   NNP   noun, proper singular\n",
            "flew   VERB   verb   VBD   verb, past tense\n",
            "to   ADP   adposition   IN   conjunction, subordinating or preposition\n",
            "mars   NOUN   noun   NNS   noun, plural\n",
            "yesterday   NOUN   noun   NN   noun, singular or mass\n",
            ".   PUNCT   punctuation   .   punctuation mark, sentence closer\n",
            "He   PRON   pronoun   PRP   pronoun, personal\n",
            "carried   VERB   verb   VBD   verb, past tense\n",
            "biryani   ADJ   adjective   JJ   adjective (English), other noun-modifier (Chinese)\n",
            "masala   NOUN   noun   NN   noun, singular or mass\n",
            "with   ADP   adposition   IN   conjunction, subordinating or preposition\n",
            "him   PRON   pronoun   PRP   pronoun, personal\n"
          ]
        }
      ],
      "source": [
        "# knowing whether a verb is a past tense, e.g in our sentence the word flew\n",
        "for words in doc:\n",
        "  print(words,\" \", words.pos_,\" \" ,spacy.explain(words.pos_),\" \", words.tag_,\" \", spacy.explain(words.tag_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJwwlSozYt8b"
      },
      "source": [
        "# **Name Entity Recognition**\n",
        "\n",
        " it is used in recognizing entities  such as company, product , people, money etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNEIuE4fx9m4"
      },
      "outputs": [],
      "source": [
        "nlp=spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MujMtbk4Kcvd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9b5dede-c453-418a-a835-ad98d1fadc9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# in the pipeline there  is ner\n",
        "nlp.pipe_names"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we use the explanation to see the expanded version of the labels\n",
        "doc2=nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
        "for entities in doc2.ents:\n",
        "  print(entities.text, \"\", entities.label_, \"\", spacy.explain(entities.label_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FPEYKq2NZLj",
        "outputId": "8873977f-eec6-4942-d99d-348cabe79be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc  ORG  Companies, agencies, institutions, etc.\n",
            "$45 billion  MONEY  Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The word twitter is not recognized as a company, as from our sentences the word twitter is in small letter, therefore spacy uses some rules like capitalization of entities, also presence of Inc, so thats why it was not recognized a first place"
      ],
      "metadata": {
        "id": "lqtKdDHpNhns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc2=nlp(\"Tesla Inc is going to acquire Twitter for $45 billion\")\n",
        "for entities in doc2.ents:\n",
        "  print(entities.text, \"\", entities.label_, \"\", spacy.explain(entities.label_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPz6LDasLal9",
        "outputId": "96d51d0f-5532-4d6d-d2ab-daeaba662d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc  ORG  Companies, agencies, institutions, etc.\n",
            "Twitter  PRODUCT  Objects, vehicles, foods, etc. (not services)\n",
            "$45 billion  MONEY  Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "displacy.render(doc2,style=\"ent\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "eKS44P5qMBw4",
        "outputId": "5925f85b-d6da-4ef4-d088-55982d591411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tesla Inc\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is going to acquire \n",
              "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Twitter\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
              "</mark>\n",
              " for \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $45 billion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can be able to see what  spacy NER support\n",
        "nlp.pipe_labels['ner']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZTVEXRoMtPH",
        "outputId": "634fc622-d558-4fda-a32f-085efe92547d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['CARDINAL',\n",
              " 'DATE',\n",
              " 'EVENT',\n",
              " 'FAC',\n",
              " 'GPE',\n",
              " 'LANGUAGE',\n",
              " 'LAW',\n",
              " 'LOC',\n",
              " 'MONEY',\n",
              " 'NORP',\n",
              " 'ORDINAL',\n",
              " 'ORG',\n",
              " 'PERCENT',\n",
              " 'PERSON',\n",
              " 'PRODUCT',\n",
              " 'QUANTITY',\n",
              " 'TIME',\n",
              " 'WORK_OF_ART']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc3=nlp(\" Dr. muguro is a cool guy at Dekut in Nyeri  in  2024, working  with him  in project like nLP but not good in data collection\")\n",
        "for ent in doc3.ents:\n",
        "  print(ent.text, \"\", ent.label_, \"\", spacy.explain(ent.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ivIHhMQOXYn",
        "outputId": "3142f957-da3b-43db-9ac2-1d5c8ce41780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "muguro  PERSON  People, including fictional\n",
            "Dekut  ORG  Companies, agencies, institutions, etc.\n",
            "Nyeri  GPE  Countries, cities, states\n",
            "2024  DATE  Absolute or relative dates or periods\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "in spacy for an entity to be recognized it must start with a capital letter which is  a limitation"
      ],
      "metadata": {
        "id": "vKRcM346P1Ev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Representation**"
      ],
      "metadata": {
        "id": "Nea0HHA9WdEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. stop words\n",
        "\n",
        "they are some words which are neccesary and at other times they are not neccesary, this depends on problem statement"
      ],
      "metadata": {
        "id": "DbNKCG7AWo2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS"
      ],
      "metadata": {
        "id": "B9bgNlXJO4nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#these are commonly used stop words\n",
        "STOP_WORDS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNeqs8xvFh80",
        "outputId": "26dfc7f4-5950-4af2-f502-ddd7ae3a68bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"'d\",\n",
              " \"'ll\",\n",
              " \"'m\",\n",
              " \"'re\",\n",
              " \"'s\",\n",
              " \"'ve\",\n",
              " 'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'across',\n",
              " 'after',\n",
              " 'afterwards',\n",
              " 'again',\n",
              " 'against',\n",
              " 'all',\n",
              " 'almost',\n",
              " 'alone',\n",
              " 'along',\n",
              " 'already',\n",
              " 'also',\n",
              " 'although',\n",
              " 'always',\n",
              " 'am',\n",
              " 'among',\n",
              " 'amongst',\n",
              " 'amount',\n",
              " 'an',\n",
              " 'and',\n",
              " 'another',\n",
              " 'any',\n",
              " 'anyhow',\n",
              " 'anyone',\n",
              " 'anything',\n",
              " 'anyway',\n",
              " 'anywhere',\n",
              " 'are',\n",
              " 'around',\n",
              " 'as',\n",
              " 'at',\n",
              " 'back',\n",
              " 'be',\n",
              " 'became',\n",
              " 'because',\n",
              " 'become',\n",
              " 'becomes',\n",
              " 'becoming',\n",
              " 'been',\n",
              " 'before',\n",
              " 'beforehand',\n",
              " 'behind',\n",
              " 'being',\n",
              " 'below',\n",
              " 'beside',\n",
              " 'besides',\n",
              " 'between',\n",
              " 'beyond',\n",
              " 'both',\n",
              " 'bottom',\n",
              " 'but',\n",
              " 'by',\n",
              " 'ca',\n",
              " 'call',\n",
              " 'can',\n",
              " 'cannot',\n",
              " 'could',\n",
              " 'did',\n",
              " 'do',\n",
              " 'does',\n",
              " 'doing',\n",
              " 'done',\n",
              " 'down',\n",
              " 'due',\n",
              " 'during',\n",
              " 'each',\n",
              " 'eight',\n",
              " 'either',\n",
              " 'eleven',\n",
              " 'else',\n",
              " 'elsewhere',\n",
              " 'empty',\n",
              " 'enough',\n",
              " 'even',\n",
              " 'ever',\n",
              " 'every',\n",
              " 'everyone',\n",
              " 'everything',\n",
              " 'everywhere',\n",
              " 'except',\n",
              " 'few',\n",
              " 'fifteen',\n",
              " 'fifty',\n",
              " 'first',\n",
              " 'five',\n",
              " 'for',\n",
              " 'former',\n",
              " 'formerly',\n",
              " 'forty',\n",
              " 'four',\n",
              " 'from',\n",
              " 'front',\n",
              " 'full',\n",
              " 'further',\n",
              " 'get',\n",
              " 'give',\n",
              " 'go',\n",
              " 'had',\n",
              " 'has',\n",
              " 'have',\n",
              " 'he',\n",
              " 'hence',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hereafter',\n",
              " 'hereby',\n",
              " 'herein',\n",
              " 'hereupon',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'however',\n",
              " 'hundred',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'indeed',\n",
              " 'into',\n",
              " 'is',\n",
              " 'it',\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'keep',\n",
              " 'last',\n",
              " 'latter',\n",
              " 'latterly',\n",
              " 'least',\n",
              " 'less',\n",
              " 'made',\n",
              " 'make',\n",
              " 'many',\n",
              " 'may',\n",
              " 'me',\n",
              " 'meanwhile',\n",
              " 'might',\n",
              " 'mine',\n",
              " 'more',\n",
              " 'moreover',\n",
              " 'most',\n",
              " 'mostly',\n",
              " 'move',\n",
              " 'much',\n",
              " 'must',\n",
              " 'my',\n",
              " 'myself',\n",
              " \"n't\",\n",
              " 'name',\n",
              " 'namely',\n",
              " 'neither',\n",
              " 'never',\n",
              " 'nevertheless',\n",
              " 'next',\n",
              " 'nine',\n",
              " 'no',\n",
              " 'nobody',\n",
              " 'none',\n",
              " 'noone',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'nothing',\n",
              " 'now',\n",
              " 'nowhere',\n",
              " 'n‘t',\n",
              " 'n’t',\n",
              " 'of',\n",
              " 'off',\n",
              " 'often',\n",
              " 'on',\n",
              " 'once',\n",
              " 'one',\n",
              " 'only',\n",
              " 'onto',\n",
              " 'or',\n",
              " 'other',\n",
              " 'others',\n",
              " 'otherwise',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 'part',\n",
              " 'per',\n",
              " 'perhaps',\n",
              " 'please',\n",
              " 'put',\n",
              " 'quite',\n",
              " 'rather',\n",
              " 're',\n",
              " 'really',\n",
              " 'regarding',\n",
              " 'same',\n",
              " 'say',\n",
              " 'see',\n",
              " 'seem',\n",
              " 'seemed',\n",
              " 'seeming',\n",
              " 'seems',\n",
              " 'serious',\n",
              " 'several',\n",
              " 'she',\n",
              " 'should',\n",
              " 'show',\n",
              " 'side',\n",
              " 'since',\n",
              " 'six',\n",
              " 'sixty',\n",
              " 'so',\n",
              " 'some',\n",
              " 'somehow',\n",
              " 'someone',\n",
              " 'something',\n",
              " 'sometime',\n",
              " 'sometimes',\n",
              " 'somewhere',\n",
              " 'still',\n",
              " 'such',\n",
              " 'take',\n",
              " 'ten',\n",
              " 'than',\n",
              " 'that',\n",
              " 'the',\n",
              " 'their',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'thence',\n",
              " 'there',\n",
              " 'thereafter',\n",
              " 'thereby',\n",
              " 'therefore',\n",
              " 'therein',\n",
              " 'thereupon',\n",
              " 'these',\n",
              " 'they',\n",
              " 'third',\n",
              " 'this',\n",
              " 'those',\n",
              " 'though',\n",
              " 'three',\n",
              " 'through',\n",
              " 'throughout',\n",
              " 'thru',\n",
              " 'thus',\n",
              " 'to',\n",
              " 'together',\n",
              " 'too',\n",
              " 'top',\n",
              " 'toward',\n",
              " 'towards',\n",
              " 'twelve',\n",
              " 'twenty',\n",
              " 'two',\n",
              " 'under',\n",
              " 'unless',\n",
              " 'until',\n",
              " 'up',\n",
              " 'upon',\n",
              " 'us',\n",
              " 'used',\n",
              " 'using',\n",
              " 'various',\n",
              " 'very',\n",
              " 'via',\n",
              " 'was',\n",
              " 'we',\n",
              " 'well',\n",
              " 'were',\n",
              " 'what',\n",
              " 'whatever',\n",
              " 'when',\n",
              " 'whence',\n",
              " 'whenever',\n",
              " 'where',\n",
              " 'whereafter',\n",
              " 'whereas',\n",
              " 'whereby',\n",
              " 'wherein',\n",
              " 'whereupon',\n",
              " 'wherever',\n",
              " 'whether',\n",
              " 'which',\n",
              " 'while',\n",
              " 'whither',\n",
              " 'who',\n",
              " 'whoever',\n",
              " 'whole',\n",
              " 'whom',\n",
              " 'whose',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'within',\n",
              " 'without',\n",
              " 'would',\n",
              " 'yet',\n",
              " 'you',\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " '‘d',\n",
              " '‘ll',\n",
              " '‘m',\n",
              " '‘re',\n",
              " '‘s',\n",
              " '‘ve',\n",
              " '’d',\n",
              " '’ll',\n",
              " '’m',\n",
              " '’re',\n",
              " '’s',\n",
              " '’ve'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# how many number of stop words that are there\n",
        "len(STOP_WORDS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H03W0fp4Fl1v",
        "outputId": "972a156e-70ea-4644-8f06-bef6676e5131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "326"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#gettting the stop words in a text/sentence\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(\"the money have just been raised, what!, then how long will i wait for payment to be done, hopefully by end of this week \")\n",
        "for words in doc:\n",
        "  if words.is_stop:\n",
        "    print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD9-AYklGAe8",
        "outputId": "d5c4a57f-5be6-4d59-c2af-cd7c79bac2a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the\n",
            "have\n",
            "just\n",
            "been\n",
            "what\n",
            "then\n",
            "how\n",
            "will\n",
            "i\n",
            "for\n",
            "to\n",
            "be\n",
            "done\n",
            "by\n",
            "of\n",
            "this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using a function  to print non stop words\n",
        "def preprocess(text):\n",
        "  doc=nlp(text)\n",
        "  not_stop_words=[words.text for words in doc if  not words.is_stop]\n",
        "  return not_stop_words"
      ],
      "metadata": {
        "id": "KeWknGjvGNfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRrx9xiXIQzT",
        "outputId": "2073f70c-a858-448f-f900-dc81c078ed85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['money',\n",
              " 'raised',\n",
              " ',',\n",
              " '!',\n",
              " ',',\n",
              " 'long',\n",
              " 'wait',\n",
              " 'payment',\n",
              " ',',\n",
              " 'hopefully',\n",
              " 'end',\n",
              " 'week']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# **2. Bag of words**"
      ],
      "metadata": {
        "id": "1rLggAg6916p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "v=CountVectorizer()\n",
        "v.fit([\"mary is a hardworking and blessed girl\"])\n",
        "v.vocabulary_"
      ],
      "metadata": {
        "id": "CrEPRBxFmt7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f29b164-fa6d-47c7-cfba-43260333123c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mary': 5, 'is': 4, 'hardworking': 3, 'and': 0, 'blessed': 1, 'girl': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#however you cann use bi-grams,tri-grams\n",
        "x=CountVectorizer(ngram_range=(1,2))\n",
        "x.fit([\"Mary is starting  her masters program in january\"])\n",
        "x.vocabulary_\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUBDTwtP-bSK",
        "outputId": "4126f8ff-34ea-466d-e791-de2dbcb180a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mary': 7,\n",
              " 'is': 4,\n",
              " 'starting': 13,\n",
              " 'her': 0,\n",
              " 'masters': 9,\n",
              " 'program': 11,\n",
              " 'in': 2,\n",
              " 'january': 6,\n",
              " 'mary is': 8,\n",
              " 'is starting': 5,\n",
              " 'starting her': 14,\n",
              " 'her masters': 1,\n",
              " 'masters program': 10,\n",
              " 'program in': 12,\n",
              " 'in january': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=CountVectorizer(ngram_range=(2,2))\n",
        "x.fit([\"Mary is starting  her masters program in january\"])\n",
        "x.vocabulary_\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG0JMLmC_rMx",
        "outputId": "3dfd407f-15ea-416e-8d63-51ded90064b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mary is': 3,\n",
              " 'is starting': 2,\n",
              " 'starting her': 6,\n",
              " 'her masters': 0,\n",
              " 'masters program': 4,\n",
              " 'program in': 5,\n",
              " 'in january': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=[\"mary ate pizza\",\"mary like reading\",\"mary is hardworking\"]"
      ],
      "metadata": {
        "id": "yCXTPoGUBO9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "filtered_tokens=[]\n",
        "def preprocess(text):\n",
        "  doc=nlp(text)\n",
        "  for token in doc:\n",
        "    if token.is_stop or token.is_punct:\n",
        "      continue\n",
        "    filtered_tokens.append(token.lemma_)\n",
        "  return \" \".join(filtered_tokens)\n",
        "preprocess(\"mary ate pizzza\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nRv7bxqR_79L",
        "outputId": "0695e743-2f03-4577-e7e9-04984ab38b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mary eat pizzza'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_processed=[preprocess(text) for text in corpus]\n",
        "corpus_processed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AJ3waGmBiaN",
        "outputId": "bdf36cba-2826-4259-b92e-facd17c55f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mary eat pizzza mary eat pizza',\n",
              " 'mary eat pizzza mary eat pizza mary like read',\n",
              " 'mary eat pizzza mary eat pizza mary like read mary hardworke']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v=CountVectorizer(ngram_range=(1,2))\n",
        "v.fit(corpus_processed)\n",
        "v.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZpjiQZoCNyv",
        "outputId": "8eb0b9aa-51d5-4f64-db20-c0b8bded084d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mary': 6,\n",
              " 'eat': 0,\n",
              " 'pizzza': 12,\n",
              " 'pizza': 10,\n",
              " 'mary eat': 7,\n",
              " 'eat pizzza': 2,\n",
              " 'pizzza mary': 13,\n",
              " 'eat pizza': 1,\n",
              " 'like': 4,\n",
              " 'read': 14,\n",
              " 'pizza mary': 11,\n",
              " 'mary like': 9,\n",
              " 'like read': 5,\n",
              " 'hardworke': 3,\n",
              " 'read mary': 15,\n",
              " 'mary hardworke': 8}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert the text in vector using BOW\n",
        "v.transform([\"mary eats pizza\"]).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XSKXv6WCNv7",
        "outputId": "5b785f27-73de-4a60-99e6-f7f6ed91b8ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. TF-IDF**"
      ],
      "metadata": {
        "id": "7BrYhQGrFHAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus_2= [\"mary is reading a book\",\" she is busy over the weekend\",\"Mary is a hardworking and blessed girl\"]\n"
      ],
      "metadata": {
        "id": "H_RQ2OlJCNpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the output shows thenumber based on indx as the appear alphabetically\n",
        "vect=TfidfVectorizer()\n",
        "transformed=vect.fit(corpus_2)\n",
        "vect.vocabulary_"
      ],
      "metadata": {
        "id": "SGuwECfuCNmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed335d48-f1e2-4273-ff2c-c4567d5b2757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mary': 7,\n",
              " 'is': 6,\n",
              " 'reading': 9,\n",
              " 'book': 2,\n",
              " 'she': 10,\n",
              " 'busy': 3,\n",
              " 'over': 8,\n",
              " 'the': 11,\n",
              " 'weekend': 12,\n",
              " 'hardworking': 5,\n",
              " 'and': 0,\n",
              " 'blessed': 1,\n",
              " 'girl': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#getting the names in order\n",
        "all_features=vect.get_feature_names_out()\n",
        "all_features"
      ],
      "metadata": {
        "id": "aPbmxc_SCNjG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "742d1d73-cfdb-4711-c2fd-3c7396469dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['and', 'blessed', 'book', 'busy', 'girl', 'hardworking', 'is',\n",
              "       'mary', 'over', 'reading', 'she', 'the', 'weekend'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for words in all_features:\n",
        "  index=vect.vocabulary_.get(words)\n",
        "  print(f\"{words}{vect.idf_[index]}\")\n"
      ],
      "metadata": {
        "id": "tdNX46MsCNgj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b32c219-7378-4b76-f73d-69ed60957281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "and1.6931471805599454\n",
            "blessed1.6931471805599454\n",
            "book1.6931471805599454\n",
            "busy1.6931471805599454\n",
            "girl1.6931471805599454\n",
            "hardworking1.6931471805599454\n",
            "is1.0\n",
            "mary1.2876820724517808\n",
            "over1.6931471805599454\n",
            "reading1.6931471805599454\n",
            "she1.6931471805599454\n",
            "the1.6931471805599454\n",
            "weekend1.6931471805599454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the word **\"is\"** has been appearing several times meaning its a generic term, where therefore its score is very low"
      ],
      "metadata": {
        "id": "N6pm6LDbD7BW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yA0sZO0TEVS6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNt5qMoxyWaXpz8l1CwJFM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}